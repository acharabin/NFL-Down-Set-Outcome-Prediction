{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Neural Network Framework to Predict NFL Down Set Outcome Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from importlib import reload\n",
    "import dnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload utils if necessary\n",
    "dnn_utils=reload(dnn_utils)\n",
    "from dnn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Preparation Part 1\n",
    "Initial data cleaning, labelling, and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NFL Play by Play 2009-2018 (v5).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Source - https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016?resource=download\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNFL Play by Play 2009-2018 (v5).csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Python310\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NFL Play by Play 2009-2018 (v5).csv'"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "# Source - https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016?resource=download\n",
    "\n",
    "dataset=pd.read_csv(\"NFL Play by Play 2009-2018 (v5).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Observe First Records\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Columns\n",
    "list(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates and Erroneous/Irrelevant Records\n",
    "\n",
    "dataset=dataset[dataset.duplicated()==False]\n",
    "dataset=dataset[dataset['down'].isnull()==False]\n",
    "dataset=dataset[dataset['yardline_100'].isnull()==False]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yardline field that adjusts yardline_100 into Buckets of 10\n",
    "yardline_buckets=10\n",
    "dataset['yardline']=np.ceil(dataset['yardline_100']/yardline_buckets)*yardline_buckets\n",
    "dataset['yardline'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yards to go field with a final bucket of 11+\n",
    "dataset['ydstogo_archive']=dataset['ydstogo']\n",
    "yards_to_go_max=11\n",
    "dataset['ydstogo']=np.where(dataset['ydstogo']>yards_to_go_max,yards_to_go_max,dataset['ydstogo'])\n",
    "dataset['ydstogo'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert yards_gained to Int64\n",
    "dataset['yards_gained_archive']=dataset['yards_gained']\n",
    "dataset['yards_gained']=dataset['yards_gained'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the lack on monotonicity observed in some cases with play_id >\n",
    "# in the loop, I will rebuild the play_id to enumerate from 1:p in a game based on the dataset order\n",
    "dataset['play_id_archive']=dataset['play_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a drive_play feature that enumerates plays from 1:n in each drive.\n",
    "# 2. A down set can only last 4 downs so we only need to look within +/- 3 entries to find other plays in the down set. \n",
    "# 2. I can run a loop through all games, drives, and plays that adds a down each time there is a new down 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find down set through a loop\n",
    "# Likely can't be vectorized as finding the drive set requires cumulative information from the drive\n",
    "dataset['down']=dataset['down'].apply(int)\n",
    "down_set_list=[]\n",
    "down_set_id_list=[]\n",
    "down_set_id=0\n",
    "play_in_game_list=[]\n",
    "for g, game_id in enumerate(dataset['game_id'].unique(), start=1):\n",
    "    game_dataset=dataset[dataset['game_id']==game_id]\n",
    "    play_in_game=1\n",
    "    for d, drive in enumerate(game_dataset['drive'].unique(), start=1):\n",
    "        drive_dataset=game_dataset[game_dataset['drive']==drive]\n",
    "        for p, play_id in enumerate(drive_dataset['play_id'].unique(), start=1):\n",
    "            play_in_game+=1\n",
    "            if p==1: \n",
    "                down_set=1\n",
    "                down_set_id+=1\n",
    "            elif drive_dataset.iloc[p-1,:]['down']==1:\n",
    "                down_set+=1\n",
    "            down_set_list.append(down_set)\n",
    "            down_set_id_list.append(down_set_id)\n",
    "            play_in_game_list.append(play_in_game)\n",
    "dataset['down_set']=down_set_list\n",
    "dataset['down_set_id']=down_set_id_list\n",
    "dataset['play_id']=play_in_game_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe Result\n",
    "dataset.loc[dataset['game_id']==2009091000,['game_id','drive','down','down_set','posteam']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build requirements to find absorption states\n",
    "\n",
    "# Create identifier for the last play in a down set\n",
    "dataset['down_set_max_play']=dataset.groupby(['game_id','drive','down_set'])['play_id'].transform(max)\n",
    "\n",
    "# Create identifier for the last play in a drive\n",
    "dataset['drive_max_play']=dataset.groupby(['game_id','drive'])['play_id'].transform(max)\n",
    "\n",
    "# Create identifier for the last play in a game\n",
    "dataset['game_half_max_play']=dataset.groupby(['game_id','game_half'])['play_id'].transform(max)\n",
    "\n",
    "# Find yardline of next down_set\n",
    "# dataset=dataset.drop(['yardline_next_down_set'],axis=1)\n",
    "first_down=dataset.loc[dataset['down']==1,['game_id','drive','down_set','yardline']].rename(columns={'yardline': 'yardline_next_down_set'})\n",
    "first_down['down_set']=first_down['down_set']-1\n",
    "dataset=dataset.merge(first_down,how='left',on=['game_id','drive','down_set'])\n",
    "\n",
    "# Filter to the last down in the down_set\n",
    "last_play_in_down_set=dataset[dataset['play_id']==dataset['down_set_max_play']]\n",
    "\n",
    "# Observe Result\n",
    "dataset.loc[dataset['game_id']==2009091000,['game_id','drive','down','down_set','play_id','yardline','posteam','down_set_max_play','drive_max_play','game_half_max_play','yardline_next_down_set']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the absorption state of the down set\n",
    "\n",
    "# 19 different options\n",
    "# 10 options where the drive continues\n",
    "# One for each yardline position\n",
    "# 9 options where the drive ends\n",
    "# touchdown, field goal, safety, missed field goal, fumble, interception, turnover on downs, punt, end of half or game.\n",
    "\n",
    "last_play_in_down_set['absorption_state']=np.where(last_play_in_down_set['play_id']!=last_play_in_down_set['drive_max_play'],last_play_in_down_set['yardline_next_down_set'].apply(str),\n",
    "                                              np.where(last_play_in_down_set['touchdown']==1,'touchdown',np.where(last_play_in_down_set['field_goal_result']=='made','field_goal',np.where(last_play_in_down_set['safety']==1,'safety',np.where(last_play_in_down_set['field_goal_result'].isin(['missed','blocked']),'missed_field_goal',np.where(last_play_in_down_set['fumble']==1,'fumble',np.where(last_play_in_down_set['interception']==1,'interception',np.where(last_play_in_down_set['kick_distance'].isnull()==False,'punt',np.where(last_play_in_down_set['play_id']==last_play_in_down_set['game_half_max_play'],'end_of_half','turnover')))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe down set absorption states\n",
    "\n",
    "print(last_play_in_down_set['absorption_state'].isnull().value_counts())\n",
    "print(len(last_play_in_down_set['absorption_state'].unique()))\n",
    "last_play_in_down_set['absorption_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe results for a game\n",
    "\n",
    "# last_play_in_down_set.loc[(last_play_in_down_set['absorption_state']=='nan'),['game_id','drive','down','down_set','play_id','down_set_max_play','drive_max_play','game_half_max_play','yardline','posteam','yardline_next_down_set','touchdown','field_goal_result','safety','fumble','interception','kick_distance','half_seconds_remaining','game_seconds_remaining','ydstogo_archive','yards_gained','penalty','absorption_state']].head(20)\n",
    "last_play_in_down_set.loc[last_play_in_down_set['game_id']==2009091000,['game_id','drive','down','down_set','play_id','down_set_max_play','drive_max_play','game_half_max_play','yardline','posteam','yardline_next_down_set','touchdown','field_goal_result','safety','fumble','interception','kick_distance','half_seconds_remaining','game_seconds_remaining','ydstogo_archive','yards_gained','penalty','absorption_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join absorption_state to the dataset with all plays\n",
    "\n",
    "dataset_len_before=len(dataset)\n",
    "dataset=dataset.merge(last_play_in_down_set.loc[:,['game_id','drive','down_set','absorption_state']],how='left',on=['game_id','drive','down_set'])\n",
    "assert len(dataset) == dataset_len_before\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only required columns\n",
    "\n",
    "# dataset=dataset_archive\n",
    "dataset_archive=dataset\n",
    "\n",
    "dataset=dataset.loc[:,['game_id','game_date','drive','down_set','down_set_id','play_id','down','yardline','ydstogo','absorption_state','half_seconds_remaining']]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset after part 1 data preparation\n",
    "dataset_dataprep_part1=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset_dataprep_part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Data Preparation Part 2\n",
    "Preparing data for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale half_seconds remaining over total seconds in half of 1800\n",
    "dataset['half_seconds_remaining']=dataset['half_seconds_remaining']/1800\n",
    "print(dataset['half_seconds_remaining'].max())\n",
    "print(dataset['half_seconds_remaining'].min())\n",
    "dataset['half_seconds_remaining'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a matrix with the intersection of yardline and ydstogo\n",
    "# In the future, look to vectorize the for loop\n",
    "\n",
    "innerstates = np.zeros([10,11,len(dataset)])\n",
    "\n",
    "for i in range(innerstates.shape[2]):\n",
    "    innerstates[(dataset['yardline']/10).astype(int)[i]-1,dataset['ydstogo'][i]-1,i]=1\n",
    "\n",
    "innerstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe results\n",
    "innerstates[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll the matrix horizontally\n",
    "innerstates_reshaped=innerstates.reshape(innerstates.shape[0]*innerstates.shape[1],innerstates.shape[2])\n",
    "innerstates_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify horizontal unrolling & observe results\n",
    "i=0\n",
    "yardline=(dataset['yardline']/10).astype(int)[i]\n",
    "ydstogo=dataset['ydstogo'][i]\n",
    "# print(yardline)\n",
    "# print(ydstogo)\n",
    "print((yardline-1)*11+ydstogo-1)\n",
    "assert ((yardline-1)*11+ydstogo-1) == list(innerstates_reshaped[:,i]).index(1)\n",
    "innerstates_reshaped[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build absorption states\n",
    "# Build a matrix with the intersection of yardline and ydstogo\n",
    "# In the future, look to vectorize the for loop\n",
    "\n",
    "print(dataset['absorption_state'].unique())\n",
    "\n",
    "absorption_states=['10.0', '20.0', '30.0', '40.0', '50.0', '60.0', '70.0', '80.0', '90.0', '100.0',\n",
    " 'punt', 'missed_field_goal', 'interception', 'touchdown', 'fumble', 'field_goal', 'end_of_half', 'turnover', 'safety']\n",
    "\n",
    "print(absorption_states)\n",
    "\n",
    "absorption_states_reshaped = np.zeros([len(absorption_states),len(dataset)])\n",
    "\n",
    "absorption_states_reshaped\n",
    "\n",
    "for i in range(absorption_states_reshaped.shape[1]):\n",
    "    absorption_states_reshaped[absorption_states.index(dataset['absorption_state'][i]),i]=1\n",
    "\n",
    "absorption_states_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe results\n",
    "print(absorption_states.index(dataset['absorption_state'][i]))\n",
    "list(absorption_states_reshaped[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into training, hypertuning, and testing\n",
    "# I will use 2009-2016, 2017, and 2018, for each respectively\n",
    "dataset['partition']=np.where(pd.DatetimeIndex(dataset['game_date']).year==2018,'testing',np.where(pd.DatetimeIndex(dataset['game_date']).year==2017,'hypertuning','training'))\n",
    "# Games by partition\n",
    "print(dataset.groupby(['partition']).game_id.nunique().sort_values(ascending=False))\n",
    "print(dataset.groupby(['partition']).down_set_id.nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "# identifiers (3) + innerstates (110) + features (1) + absorptionstates (19) = \n",
    "dataset_reshaped=pd.concat([dataset.loc[:,['down_set_id','down','partition']],pd.DataFrame(innerstates_reshaped.T),dataset.loc[:,['half_seconds_remaining']],pd.DataFrame(absorption_states_reshaped.T)],axis=1)\n",
    "print(dataset_reshaped.shape)\n",
    "dataset_reshaped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each partition\n",
    "training_withid=dataset_reshaped[dataset_reshaped['partition']=='training'].drop(['partition'],axis=1)\n",
    "hypertuning_withid=dataset_reshaped[dataset_reshaped['partition']=='hypertuning'].drop(['partition'],axis=1)\n",
    "testing_withid=dataset_reshaped[dataset_reshaped['partition']=='testing'].drop(['partition'],axis=1)\n",
    "\n",
    "training=training_withid.drop(['down_set_id'],axis=1)\n",
    "hypertuning=hypertuning_withid.drop(['down_set_id'],axis=1)\n",
    "testing=testing_withid.drop(['down_set_id'],axis=1)\n",
    "\n",
    "training_x=np.array(training.iloc[:,0:(len(training.columns)-19)]).T\n",
    "hypertuning_x=np.array(hypertuning.iloc[:,0:(len(hypertuning.columns)-19)]).T\n",
    "testing_x=np.array(testing.iloc[:,0:(len(testing.columns)-19)]).T\n",
    "\n",
    "training_y=np.array(training.iloc[:,(len(training.columns)-19):len(training.columns)]).T\n",
    "hypertuning_y=np.array(hypertuning.iloc[:,(len(hypertuning.columns)-19):len(hypertuning.columns)]).T\n",
    "testing_y=np.array(testing.iloc[:,(len(testing.columns)-19):len(testing.columns)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe training records by down set\n",
    "print(dataset.groupby(['down']).down_set_id.nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shapes\n",
    "# shape = (n_x/n_y, m training examples)\n",
    "print(training_x.shape)\n",
    "print(training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can get data for a down as follows\n",
    "down=4\n",
    "print(training_x[:,training_x[0,:]==down][1:,:].shape)\n",
    "training_x[:,training_x[0,:]==down][1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to save params locally\n",
    "\n",
    "def save_params(parameters,path = 'modelparams'):\n",
    "    time=str(datetime.now())[0:10]+'--'+str(datetime.now())[11:13]+'-'+str(datetime.now())[14:16]\n",
    "    filename = f\"\"\"params-{time}.npy\"\"\"\n",
    "    np.save(os.path.join(path,filename),  parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(filename, path = \"modelparams\"):\n",
    "    parameters=np.load(os.path.join(path,filename),allow_pickle=True)\n",
    "    return np.array(parameters, ndmin=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_last_params(path = \"modelparams\"):\n",
    "    for i, filename in enumerate(reversed(os.listdir(path))):\n",
    "        if i==0: parameters=np.load(os.path.join(path,filename),allow_pickle=True)\n",
    "        return np.array(parameters, ndmin=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, parameters, learning_rate = 0.001, batch_size = 1000, num_iterations = 3000, down = 4, print_cost = False, last_layer_activation = 'softmax'):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->last_layer_activation.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector of absorption states (contains 1 for the actual absorption state and 0 otherwise), of shape (19, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Layer Count\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Subset full down data to only the specified down\n",
    "    \n",
    "    matching_down = X[0,:]==down\n",
    "        \n",
    "    X = X[:,matching_down][1:,:]\n",
    "    Y = Y[:,matching_down]\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Define mini batch function\n",
    "        \n",
    "    def random_mini_batches(X,Y,minibatch_size):\n",
    "\n",
    "        m = Y.shape[1]            # number of examples\n",
    "\n",
    "        # Lets shuffle X and Y\n",
    "        permutation = list(np.random.permutation(m))            # shuffled index of examples\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation]\n",
    "\n",
    "        minibatches = []                                        # we will append all minibatch_Xs and minibatch_Ys to this minibatch list \n",
    "        number_of_minibatches = int(m/minibatch_size)           # number of mini batches \n",
    "\n",
    "        for k in range(number_of_minibatches):\n",
    "            minibatch_X = shuffled_X[:,k*minibatch_size: (k+1)*minibatch_size ]\n",
    "            minibatch_Y = shuffled_Y[:,k*minibatch_size: (k+1)*minibatch_size ]\n",
    "            minibatch_pair = (minibatch_X , minibatch_Y)                        #tuple of minibatch_X and miinibatch_Y\n",
    "            minibatches.append(minibatch_pair)\n",
    "        if m%minibatch_size != 0 :\n",
    "            last_minibatch_X = shuffled_X[:,(k+1)*minibatch_size: m ]\n",
    "            last_minibatch_Y = shuffled_Y[:,(k+1)*minibatch_size: m ]\n",
    "            last_minibatch_pair = (last_minibatch_X , last_minibatch_Y)\n",
    "            minibatches.append(last_minibatch_pair)\n",
    "        return minibatches\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        batches = random_mini_batches(X, Y, batch_size)\n",
    "        \n",
    "        for i, batch in enumerate(batches):\n",
    "                    \n",
    "            X_b, Y_b = batch\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> last_layer_activation.\n",
    "            AL, caches = L_model_forward(X_b, parameters, last_layer_activation, down)\n",
    "\n",
    "            # Return results for debugging\n",
    "    #         return AL, Y, caches\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, Y_b, last_layer_activation)\n",
    "            \n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, Y_b, caches, last_layer_activation, L, down)\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate, down)\n",
    "\n",
    "            # Print the cost every 100 iterations\n",
    "            if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            if i % 100 == 0 or i == num_iterations:\n",
    "                costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims=(111,111,111,111,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization.\n",
    "parameters = initialize_parameters_deep(layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=load_last_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Single Run\n",
    "# down=4\n",
    "# parameters_test, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "\n",
    "# print(\"Cost after first iteration: \" + str(costs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.6907273657412941\n"
     ]
    }
   ],
   "source": [
    "# Train on Down 4 data with 1 layer\n",
    "for global_its in range(1):\n",
    "    for d in reversed(range(3,4)):\n",
    "        down = d + 1\n",
    "        print(f\"\"\"initiating set of iterations for down {down}\"\"\")\n",
    "        parameters, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "        print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "    save_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Down 3 data with 2 layers\n",
    "for global_its in range(10):\n",
    "    for d in reversed(range(2,3)):\n",
    "        down = d + 1\n",
    "        print(f\"\"\"initiating set of iterations for down {down}\"\"\")\n",
    "        parameters, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "        print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "    save_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters W2 to be W3\n",
    "parameters['W2']=parameters['W3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Down 2 data with 3 layers\n",
    "for global_its in range(10):\n",
    "    for d in reversed(range(1,2)):\n",
    "        down = d + 1\n",
    "        print(f\"\"\"initiating set of iterations for down {down}\"\"\")\n",
    "        parameters, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "        print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "    save_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters W1 to be W2\n",
    "parameters['W1']=parameters['W2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Down 1 data with 4 layers\n",
    "for global_its in range(10):\n",
    "    for d in reversed(range(0,1)):\n",
    "        down = d + 1\n",
    "        print(f\"\"\"initiating set of iterations for down {down}\"\"\")\n",
    "        parameters, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "        print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "    save_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.6981177381690686\n",
      "initiating set of iterations for down 3\n",
      "Cost after first iteration: -1.9352470667360313\n",
      "initiating set of iterations for down 2\n",
      "Cost after first iteration: -1.808823686567675\n",
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.6985970863124373\n",
      "initiating set of iterations for down 3\n",
      "Cost after first iteration: -1.868319395730734\n",
      "initiating set of iterations for down 2\n",
      "Cost after first iteration: -1.8016907791895451\n",
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.696117127419958\n",
      "initiating set of iterations for down 3\n",
      "Cost after first iteration: -1.87622006012111\n",
      "initiating set of iterations for down 2\n",
      "Cost after first iteration: -1.7963043668423462\n",
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.6934956805849904\n",
      "initiating set of iterations for down 3\n",
      "Cost after first iteration: -1.9155381917053242\n",
      "initiating set of iterations for down 2\n",
      "Cost after first iteration: -1.7869353511926565\n",
      "initiating set of iterations for down 4\n",
      "Cost after first iteration: -0.6908047440590229\n",
      "initiating set of iterations for down 3\n",
      "Cost after first iteration: -1.9439868123509034\n",
      "initiating set of iterations for down 2\n",
      "Cost after first iteration: -1.783083394605612\n"
     ]
    }
   ],
   "source": [
    "# Continue training on all downs\n",
    "for global_its in range(5):\n",
    "    for d in reversed(range(1,4)):\n",
    "        down = d + 1\n",
    "        print(f\"\"\"initiating set of iterations for down {down}\"\"\")\n",
    "        parameters, costs = L_layer_model(training_x, training_y, layers_dims, parameters, learning_rate = 0.001, batch_size = 100, num_iterations = 1000, down = down, print_cost = False, last_layer_activation = 'softmax')\n",
    "        print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "    save_params(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for down 4\n",
      "Accuracy: 0.7326651538073626\n",
      "Cost: -0.8900151975281239\n",
      "[['30.0' '87']\n",
      " ['40.0' '147']\n",
      " ['field_goal' '8780']\n",
      " ['missed_field_goal' '96']\n",
      " ['punt' '21644']\n",
      " ['turnover' '974']]\n",
      "Evaluation for down 3\n",
      "Accuracy: 0.40621836831095215\n",
      "Cost: -1.7920493522810965\n",
      "[['10.0' '1846']\n",
      " ['20.0' '3783']\n",
      " ['30.0' '2573']\n",
      " ['40.0' '2192']\n",
      " ['50.0' '344']\n",
      " ['60.0' '965']\n",
      " ['70.0' '850']\n",
      " ['90.0' '63']\n",
      " ['end_of_half' '283']\n",
      " ['field_goal' '4032']\n",
      " ['missed_field_goal' '24']\n",
      " ['punt' '38163']\n",
      " ['touchdown' '3682']\n",
      " ['turnover' '476']]\n",
      "Evaluation for down 2\n",
      "Accuracy: 0.38842182167119194\n",
      "Cost: -1.7112432636505381\n",
      "[['10.0' '3476']\n",
      " ['20.0' '5036']\n",
      " ['30.0' '7334']\n",
      " ['40.0' '4869']\n",
      " ['50.0' '5119']\n",
      " ['60.0' '4625']\n",
      " ['70.0' '6230']\n",
      " ['80.0' '1472']\n",
      " ['90.0' '254']\n",
      " ['end_of_half' '2112']\n",
      " ['field_goal' '7103']\n",
      " ['punt' '38183']\n",
      " ['touchdown' '5894']]\n",
      "Evaluation for down 1\n",
      "Accuracy: 0.025450299655437072\n",
      "Cost: -5.69507494582952\n",
      "[['end_of_half' '122474']]\n"
     ]
    }
   ],
   "source": [
    "# Training Set\n",
    "\n",
    "evaldata_x = training_x\n",
    "evaldata_y = training_y\n",
    "\n",
    "for d in reversed(range(4)):\n",
    "    down = d + 1\n",
    "    print(f\"\"\"Evaluation for down {down}\"\"\")\n",
    "    pred_down = predict(evaldata_x[:,evaldata_x[0,:]==down][0:,:], evaldata_y[:,evaldata_x[0,:]==down][:,:], parameters, last_layer_activation = 'softmax', down = down, return_probs = False)\n",
    "    m=evaldata_x[:,evaldata_x[0,:]==down].shape[1]\n",
    "    unique, counts = np.unique(np.tile(np.array(absorption_states,ndmin=2).T,(1, m))[pred_down==1], return_counts=True)\n",
    "    print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for down 4\n",
      "Accuracy: 0.7190909090909091\n",
      "Cost: nan\n",
      "[['30.0' '6']\n",
      " ['40.0' '12']\n",
      " ['field_goal' '948']\n",
      " ['missed_field_goal' '11']\n",
      " ['punt' '2224']\n",
      " ['turnover' '98']]\n",
      "Evaluation for down 3\n",
      "Accuracy: 0.4012872083668544\n",
      "Cost: nan\n",
      "[['10.0' '189']\n",
      " ['20.0' '419']\n",
      " ['30.0' '270']\n",
      " ['40.0' '214']\n",
      " ['50.0' '35']\n",
      " ['60.0' '113']\n",
      " ['70.0' '52']\n",
      " ['80.0' '1']\n",
      " ['90.0' '9']\n",
      " ['end_of_half' '30']\n",
      " ['field_goal' '435']\n",
      " ['missed_field_goal' '1']\n",
      " ['punt' '3953']\n",
      " ['touchdown' '426']\n",
      " ['turnover' '66']]\n",
      "Evaluation for down 2\n",
      "Accuracy: 0.3751505419510237\n",
      "Cost: nan\n",
      "[['10.0' '374']\n",
      " ['20.0' '603']\n",
      " ['30.0' '783']\n",
      " ['40.0' '568']\n",
      " ['50.0' '588']\n",
      " ['60.0' '674']\n",
      " ['70.0' '552']\n",
      " ['80.0' '156']\n",
      " ['90.0' '25']\n",
      " ['end_of_half' '235']\n",
      " ['field_goal' '789']\n",
      " ['punt' '3949']\n",
      " ['touchdown' '662']]\n",
      "Evaluation for down 1\n",
      "Accuracy: 0.025032938076416336\n",
      "Cost: nan\n",
      "[['end_of_half' '13653']]\n"
     ]
    }
   ],
   "source": [
    "# Testing Set\n",
    "\n",
    "evaldata_x = testing_x\n",
    "evaldata_y = testing_y\n",
    "\n",
    "for d in reversed(range(4)):\n",
    "    down = d + 1\n",
    "    print(f\"\"\"Evaluation for down {down}\"\"\")\n",
    "    pred_down = predict(evaldata_x[:,evaldata_x[0,:]==down][0:,:], evaldata_y[:,evaldata_x[0,:]==down][:,:], parameters, last_layer_activation = 'softmax', down = down, return_probs = False)\n",
    "    m=evaldata_x[:,evaldata_x[0,:]==down].shape[1]\n",
    "    unique, counts = np.unique(np.tile(np.array(absorption_states,ndmin=2).T,(1, m))[pred_down==1], return_counts=True)\n",
    "    print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Model Predictions with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(down, yardline, ydstogo, half_seconds_remaining, yardline_buckets=10, yards_to_go_max=11, secondsinhalf=1800):\n",
    "    \n",
    "    # Create yardline field that adjusts yardline_100 into Buckets of 10\n",
    "    yardline=np.ceil(yardline/yardline_buckets)*yardline_buckets\n",
    "\n",
    "    # Create yards to go field with a final bucket of 11+\n",
    "    ydstogo=np.where(ydstogo>yards_to_go_max,yards_to_go_max,ydstogo)\n",
    "    \n",
    "    # Scale half_seconds remaining over total seconds in half of 1800\n",
    "    half_seconds_remaining=half_seconds_remaining/secondsinhalf\n",
    "    \n",
    "    # Build a matrix with the intersection of yardline and ydstogo\n",
    "    innerstates = np.zeros([yardline_buckets,yards_to_go_max,1])\n",
    "    \n",
    "    for i in range(innerstates.shape[2]):\n",
    "        innerstates[(yardline/10).astype(int)-1,ydstogo-1,i]=1\n",
    "        \n",
    "    # Unroll the matrix horizontally\n",
    "    innerstates_reshaped=innerstates.reshape(innerstates.shape[0]*innerstates.shape[1],innerstates.shape[2])\n",
    "    \n",
    "    # Combine data\n",
    "    # Can omit concatenating down in this case as it's passed as an argument\n",
    "    data_reshaped=pd.concat([pd.DataFrame({down}),pd.DataFrame(innerstates_reshaped.T),pd.DataFrame({half_seconds_remaining})],axis=1)\n",
    "    \n",
    "    data_reshaped=np.array(data_reshaped).T\n",
    "                             \n",
    "    preds = predict(data_reshaped, None, parameters, last_layer_activation = 'softmax', down = down, return_probs = True)\n",
    "    \n",
    "    df = pd.concat([pd.DataFrame(absorption_states),pd.DataFrame(preds)],axis=1)\n",
    "    df.columns = ['absorption_state','predicted_probability']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=load_last_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absorption_state</th>\n",
       "      <th>predicted_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.014149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.014022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.008544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>punt</td>\n",
       "      <td>0.005381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>missed_field_goal</td>\n",
       "      <td>0.003435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>interception</td>\n",
       "      <td>0.045902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>touchdown</td>\n",
       "      <td>0.044157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fumble</td>\n",
       "      <td>0.017750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>field_goal</td>\n",
       "      <td>0.005304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>end_of_half</td>\n",
       "      <td>0.808083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>turnover</td>\n",
       "      <td>0.026967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>safety</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     absorption_state  predicted_probability\n",
       "0                10.0               0.014149\n",
       "1                20.0               0.014022\n",
       "2                30.0               0.008544\n",
       "3                40.0               0.003812\n",
       "4                50.0               0.001513\n",
       "5                60.0               0.000641\n",
       "6                70.0               0.000220\n",
       "7                80.0               0.000025\n",
       "8                90.0               0.000008\n",
       "9               100.0               0.000021\n",
       "10               punt               0.005381\n",
       "11  missed_field_goal               0.003435\n",
       "12       interception               0.045902\n",
       "13          touchdown               0.044157\n",
       "14             fumble               0.017750\n",
       "15         field_goal               0.005304\n",
       "16        end_of_half               0.808083\n",
       "17           turnover               0.026967\n",
       "18             safety               0.000069"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down = 1\n",
    "yardline = 10\n",
    "ydstogo = 5\n",
    "half_seconds_remaining = 250\n",
    "\n",
    "predict_new_data(down, yardline, ydstogo, half_seconds_remaining)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
